{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file to inspect its contents\n",
    "df = pd.read_csv('listings.csv')\n",
    "\n",
    "#print(df.info())\n",
    "\n",
    "# Temporarily display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Now print the head of the DataFrame with all columns visible\n",
    "#print(df.head())\n",
    "# Reset back to the default settings after displaying the data (optional)\n",
    "#pd.reset_option('display.max_columns')\n",
    "\n",
    "#print(df.columns.tolist())\n",
    "#print(len(df['amenities'].unique()))\n",
    "\n",
    "# Split the 'amenities' column into individual amenities\n",
    "df['amenities'] = df['amenities'].str.replace('[{}\"]', '', regex=True)  # Clean up the amenities\n",
    "df['amenities_list'] = df['amenities'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Flatten the list of lists into a single list of all amenities\n",
    "all_amenities = [amenity.strip() for sublist in df['amenities_list'] for amenity in sublist]\n",
    "\n",
    "# Get the number of unique amenities\n",
    "unique_amenities = set(all_amenities)\n",
    "print(f\"Total number of unique amenities: {len(unique_amenities)}\")\n",
    "\n",
    "\n",
    "# Display the 10 most common amenities\n",
    "#print(amenity_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1bde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display only columns with missing data (out of 36807 data points)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data[missing_data > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputated = pd.read_csv('listings.csv')\n",
    "\n",
    "df_imputated.drop_duplicates()\n",
    "df_imputated.drop(['license', 'calendar_updated', 'neighbourhood_group_cleansed'], axis=1, inplace=True)\n",
    "df_imputated.drop(['id', 'listing_url', 'scrape_id', 'picture_url', 'host_id', 'host_url', 'host_thumbnail_url', 'host_picture_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15774154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique values from the 'bathrooms_text' column\n",
    "unique_bathrooms_text = df['bathrooms_text'].unique()\n",
    "print(unique_bathrooms_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ab269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'number_of_reviews', 'review_scores_rating']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e52431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract bathroom numbers from the text\n",
    "def extract_bathrooms(row):\n",
    "    bathroom_text = row['bathrooms_text']\n",
    "    bathroom = row['bathrooms']\n",
    "    \n",
    "    # If both bathrooms and bathrooms_text are NaN, return NaN\n",
    "    if pd.isnull(bathroom) and pd.isnull(bathroom_text):\n",
    "        return np.nan\n",
    "    \n",
    "    # If bathrooms_text contains 'half', return 0.5\n",
    "    if isinstance(bathroom_text, str):\n",
    "        bathroom_text = bathroom_text.lower()\n",
    "        if 'half' in bathroom_text:\n",
    "            return 0.5\n",
    "        # Extract digits if they exist\n",
    "        num = ''.join([ch for ch in bathroom_text if ch.isdigit() or ch == '.'])\n",
    "        return float(num) if num else np.nan\n",
    "    \n",
    "    # Otherwise, return the existing value of bathrooms\n",
    "    return bathroom\n",
    "\n",
    "# Function to classify bathrooms into 'private', 'shared', 'no bathroom', or NaN\n",
    "def classify_bathroom(row):\n",
    "    bathrooms_text = row['bathrooms_text']\n",
    "    bathrooms = row['bathrooms']\n",
    "    \n",
    "    # If bathrooms is NaN, return NaN for category as well\n",
    "    if pd.isnull(bathrooms):\n",
    "        return 'no bathroom'\n",
    "    \n",
    "    # Convert bathrooms_text to lowercase for comparison\n",
    "    if isinstance(bathrooms_text, str):\n",
    "        text = bathrooms_text.lower()\n",
    "        \n",
    "        if bathrooms == 0:  # If bathrooms is 0, it indicates 'no bathroom'\n",
    "            return 'no bathroom'\n",
    "        elif 'shared' in text:  # If 'shared' is mentioned in text\n",
    "            return 'shared'\n",
    "        elif 'private' in text:  # If 'private' is mentioned in text\n",
    "            return 'private'\n",
    "    \n",
    "    # Default to 'private' if not explicitly mentioned\n",
    "    return 'private'\n",
    "\n",
    "df_imputated['bathrooms'] = df_imputated.apply(extract_bathrooms, axis=1)\n",
    "\n",
    "df_imputated['bathroom_category'] = df_imputated.apply(classify_bathroom, axis=1)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_imputated[['bathrooms_text', 'bathrooms', 'bathroom_category']].iloc[0: 100])\n",
    "print(df[['bathrooms_text', 'bathrooms']].iloc[0: 100])\n",
    "#pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_imputated[['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'number_of_reviews', 'review_scores_rating']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for categorical variables with 'missing' or similar labels\n",
    "df_imputated['description'].fillna('No description', inplace=True)\n",
    "df_imputated['host_about'].fillna('No host info', inplace=True)\n",
    "df_imputated['host_location'].fillna('No host location', inplace=True)\n",
    "df_imputated['bathrooms_text'].fillna('No bathroom text', inplace=True)\n",
    "df_imputated['neighborhood_overview'].fillna('No neighborhood description', inplace=True)\n",
    "\n",
    "# Fill missing review-related data with 0 (indicating no reviews)\n",
    "df_imputated['review_scores_rating'].fillna(0, inplace=True)\n",
    "df_imputated['review_scores_accuracy'].fillna(0, inplace=True)\n",
    "df_imputated['review_scores_cleanliness'].fillna(0, inplace=True)\n",
    "df_imputated['review_scores_checkin'].fillna(0, inplace=True)\n",
    "df_imputated['review_scores_communication'].fillna(0, inplace=True)\n",
    "df_imputated['review_scores_location'].fillna(0, inplace=True)\n",
    "df_imputated['review_scores_value'].fillna(0, inplace=True)\n",
    "df_imputated['reviews_per_month'].fillna(0, inplace=True)\n",
    "# Set 'first_review' and 'last_review' as NaT (Not a Timestamp) to indicate no reviews\n",
    "df_imputated['first_review'].fillna(0, inplace=True)\n",
    "df_imputated['last_review'].fillna(0, inplace=True)\n",
    "\n",
    "df_imputated.to_csv('listings_imputated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd77c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame (replace this with your actual DataFrame loading)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# 1. Fill 'host_response_time' with 'unknown'\n",
    "df_imputated['host_response_time'].fillna('unknown', inplace=True)\n",
    "\n",
    "# 2. Fill 'host_response_rate' with 0\n",
    "df_imputated['host_response_rate'].fillna(0, inplace=True)\n",
    "\n",
    "# 3. Fill 'host_acceptance_rate' with 0\n",
    "df_imputated['host_acceptance_rate'].fillna(0, inplace=True)\n",
    "\n",
    "# 4. Fill 'host_is_superhost' with False\n",
    "df_imputated['host_is_superhost'].fillna('f', inplace=True)\n",
    "\n",
    "# 5. Fill 'host_neighbourhood' with 'unknown'\n",
    "df_imputated['host_neighbourhood'].fillna('unknown', inplace=True)\n",
    "\n",
    "# 6. Fill 'neighbourhood' with 'unknown'\n",
    "df_imputated['neighbourhood'].fillna('unknown', inplace=True)\n",
    "\n",
    "# Convert 'price' to numeric (removing currency symbols and commas)\n",
    "df['price'] = pd.to_numeric(df['price'].replace({'\\$': '', ',': ''}, regex=True))\n",
    "df_imputated['price'].fillna(df['price'].median(), inplace=True)\n",
    "\n",
    "# 11. Fill 'has_availability' with False\n",
    "df_imputated['has_availability'].fillna(False, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_imputated.head())  # Check the first few rows to ensure everything worked as expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3da720",
   "metadata": {},
   "source": [
    "# Seperate Test and Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e009b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "train_set, test_set = train_test_split(df_imputated, test_size=0.2, random_state=42)\n",
    "train_set = train_set.copy()\n",
    "test_set = test_set.copy()\n",
    "\n",
    "# Check the size of the splits\n",
    "print(\"Training set size:\", len(train_set))\n",
    "print(\"Test set size:\", len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'number_of_reviews', 'review_scores_rating']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate the median values from the training set\n",
    "median_bathrooms_train = train_set['bathrooms'].median()\n",
    "median_bedrooms_train = train_set['bedrooms'].median()\n",
    "median_beds_train = train_set['beds'].median()\n",
    "\n",
    "# 3. Fill missing values in the training set using the medians from the training set\n",
    "train_set['bathrooms'].fillna(median_bathrooms_train, inplace=True)\n",
    "train_set['bedrooms'].fillna(median_bedrooms_train, inplace=True)\n",
    "train_set['beds'].fillna(median_beds_train, inplace=True)\n",
    "\n",
    "# 4. Fill missing values in the test set using the same medians from the training set\n",
    "test_set['bathrooms'].fillna(median_bathrooms_train, inplace=True)\n",
    "test_set['bedrooms'].fillna(median_bedrooms_train, inplace=True)\n",
    "test_set['beds'].fillna(median_beds_train, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be697a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'number_of_reviews', 'review_scores_rating']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884259b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_train_set = train_set.isnull().sum()\n",
    "print(f'missing_data_train_set: {missing_data_train_set[missing_data_train_set > 0]}')\n",
    "\n",
    "missing_data_test_set = test_set.isnull().sum()\n",
    "print(f'missing_data_test_set: {missing_data_test_set[missing_data_test_set > 0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gender_guesser.detector as gender\n",
    "\n",
    "\n",
    "\n",
    "# Remove '%' and convert to float\n",
    "def convert_percentage(column):\n",
    "    return column.apply(lambda x: float(str(x).rstrip('%')) / 100 if '%' in str(x) else float(x))\n",
    "\n",
    "# 7. Convert price from string to numeric by removing currency symbols\n",
    "train_set['price'] = train_set['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "test_set['price'] = test_set['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "# Optionally drop the original date columns if they are no longer needed\n",
    "train_set.drop(['last_scraped', 'host_since'], axis=1, inplace=True)\n",
    "test_set.drop(['last_scraped', 'host_since'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Drop the original 'source' and 'host_response_time' columns since they've been encoded\n",
    "train_set.drop(['source'], axis=1, inplace=True)\n",
    "test_set.drop(['source'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Drop original 'host_location' and 'host_country' columns\n",
    "train_set.drop(['host_location'], axis=1, inplace=True)\n",
    "test_set.drop(['host_location'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Drop 'host_about' as we have the length now\n",
    "train_set.drop('host_about', axis=1, inplace=True)\n",
    "test_set.drop('host_about', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Drop the original 'name' and 'description'\n",
    "train_set.drop(['name', 'description'], axis=1, inplace=True)\n",
    "test_set.drop(['name', 'description'], axis=1, inplace=True)\n",
    "\n",
    "# Drop 'neighborhood_overview'\n",
    "train_set.drop('neighborhood_overview', axis=1, inplace=True)\n",
    "test_set.drop('neighborhood_overview', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Drop 'host_name' and 'host_gender'\n",
    "train_set.drop(['host_name'], axis=1, inplace=True)\n",
    "test_set.drop(['host_name'], axis=1, inplace=True)\n",
    "  \n",
    "\n",
    "# Drop the original 'host_verifications' column\n",
    "train_set.drop(['host_verifications'], axis=1, inplace=True)\n",
    "test_set.drop(['host_verifications'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drop the original 'neighbourhood_cleansed' columns\n",
    "train_set.drop(['neighbourhood_cleansed'], axis=1, inplace=True)\n",
    "test_set.drop(['neighbourhood_cleansed'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# One-Hot Encoding for 'property_type'\n",
    "train_property_encoded = pd.get_dummies(train_set['property_type'], prefix='property_type').astype('int64')\n",
    "test_property_encoded = pd.get_dummies(test_set['property_type'], prefix='property_type').astype('int64')\n",
    "\n",
    "# One-Hot Encoding for 'room_type'\n",
    "train_room_encoded = pd.get_dummies(train_set['room_type'], prefix='room_type').astype('int64')\n",
    "test_room_encoded = pd.get_dummies(test_set['room_type'], prefix='room_type').astype('int64')\n",
    "\n",
    "# One-Hot Encoding for 'bathroom_category'\n",
    "train_bathroom_category_encoded = pd.get_dummies(train_set['bathroom_category'], prefix='bathroom_category').astype('int64')\n",
    "test_bathroom_category_encoded = pd.get_dummies(test_set['bathroom_category'], prefix='bathroom_category').astype('int64')\n",
    "\n",
    "\n",
    "# Drop the original date columns if they are no longer needed\n",
    "train_set.drop(['calendar_last_scraped', 'first_review', 'last_review'], axis=1, inplace=True)\n",
    "test_set.drop(['calendar_last_scraped', 'first_review', 'last_review'], axis=1, inplace=True)\n",
    "\n",
    "train_set.drop(['host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable', 'host_listings_count', 'host_total_listings_count', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month'], axis=1, inplace=True)\n",
    "test_set.drop(['host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable', 'host_listings_count', 'host_total_listings_count', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month'], axis=1, inplace=True)\n",
    "# Drop the original 'property_type', 'room_type', 'amenities' and bathrooms_text  columns\n",
    "train_set.drop(['property_type', 'room_type', 'amenities', 'bathrooms_text', 'bathroom_category', 'host_neighbourhood', 'neighbourhood'], axis=1, inplace=True)\n",
    "test_set.drop(['property_type', 'room_type', 'amenities', 'bathrooms_text', 'bathroom_category', 'host_neighbourhood', 'neighbourhood'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d95df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'number_of_reviews', 'review_scores_rating']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f51288",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# Display columns that are not of type int64 or float64\n",
    "non_numeric_columns = train_set.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Show the column names and their corresponding data types\n",
    "print(f'dtypes: {train_set[non_numeric_columns].dtypes}')\n",
    "print(train_set.columns.tolist())\n",
    "print(train_set.head())\n",
    "\n",
    "missing_data_train_set = train_set.isnull().sum()\n",
    "print(f'missing_data_train_set: {missing_data_train_set[missing_data_train_set > 0]}')\n",
    "missing_data_test_set = test_set.isnull().sum()\n",
    "print(f'missing_data_test_set: {missing_data_test_set[missing_data_test_set > 0]}')\n",
    "# Final processed train_set and test_set\n",
    "#print(\"Processed Training Set:\")\n",
    "#print(train_set.head())\n",
    "\n",
    "#print(\"\\nProcessed Test Set:\")\n",
    "#print(test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'number_of_reviews', 'review_scores_rating']].describe())\n",
    "# Print unique counts for bathrooms and bedrooms using a standard print statement\n",
    "print('Unique bathrooms count:\\n', train_set['bathrooms'].value_counts())\n",
    "print('Unique bedrooms count:\\n', train_set['bedrooms'].value_counts())\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of columns for which you want to create separate boxplots\n",
    "columns = ['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'number_of_reviews', 'review_scores_rating']\n",
    "\n",
    "# Loop through the list and create a boxplot for each column\n",
    "for col in columns:\n",
    "    plt.figure(figsize=(8, 4))  # Set the figure size (adjust as needed)\n",
    "    sns.boxplot(x=train_set[col])  # Create a boxplot for the current column\n",
    "    plt.title(f'Boxplot of {col}')  # Set a title for the plot\n",
    "    plt.show()  # Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b5b93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to cap outliers manually for specific columns\n",
    "def manual_cap_outliers(df, column, upper_limit):\n",
    "    df[column] = np.where(df[column] > upper_limit, upper_limit, df[column])\n",
    "    return df\n",
    "\n",
    "# Manually set reasonable upper bounds for bathrooms and bedrooms\n",
    "train_set = manual_cap_outliers(train_set, 'bathrooms', 6)  # Cap bathrooms at 5\n",
    "train_set = manual_cap_outliers(train_set, 'bedrooms', 10)   # Cap bedrooms at 5\n",
    "\n",
    "# Optionally apply the same to the test set\n",
    "test_set = manual_cap_outliers(test_set, 'bathrooms', 6)\n",
    "test_set = manual_cap_outliers(test_set, 'bedrooms', 10)\n",
    "\n",
    "# Verify the result\n",
    "print(train_set[['bathrooms', 'bedrooms']].describe())\n",
    "\n",
    "# Function to cap outliers using IQR (based on training set)\n",
    "def cap_outliers(df, column, lower_bound, upper_bound):\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "# Apply outlier capping for important numerical columns using train set bounds\n",
    "numerical_columns = ['price', 'beds']  # Add more relevant columns as needed\n",
    "\n",
    "# Calculate IQR for each column from the training set and apply the bounds\n",
    "for col in columns:\n",
    "    Q1 = train_set[col].quantile(0.25)\n",
    "    Q3 = train_set[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Apply capping to both train and test sets using the bounds from the training set\n",
    "    train_set = cap_outliers(train_set, col, lower_bound, upper_bound)\n",
    "    test_set = cap_outliers(test_set, col, lower_bound, upper_bound)\n",
    "\n",
    "# Verify the result\n",
    "print(train_set[numerical_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate lists of continuous features\n",
    "continuous_cols = ['latitude', 'longitude', 'accommodates', 'beds', 'number_of_reviews', 'review_scores_rating']\n",
    "\n",
    "# Apply StandardScaler to continuous features\n",
    "scaler_cont = StandardScaler()\n",
    "    train_set[continuous_cols] = scaler_cont.fit_transform(train_set[continuous_cols])\n",
    "test_set[continuous_cols] = scaler_cont.transform(test_set[continuous_cols])\n",
    "\n",
    "# No scaling for bathrooms and bedrooms, use them as is\n",
    "print(train_set[['bathrooms', 'bedrooms']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1502179",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_set.corr()\n",
    "print(corr_matrix['price'].sort_values(ascending=False))  # Correlation of all features with 'price'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_set and test_set have already been prepared\n",
    "\n",
    "# 1. Separate features and target\n",
    "X_train = train_set.drop('price', axis=1)\n",
    "y_train = train_set['price']\n",
    "\n",
    "X_test = test_set.drop('price', axis=1)\n",
    "y_test = test_set['price']\n",
    "\n",
    "# 2. Instantiate and train the linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions on the test set\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(lin_reg.coef_)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "# 5. Plot the residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot actual vs predicted price\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)  # Diagonal line for perfect predictions\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Use squared error for regression\n",
    "    'max_depth': 6,                   # Max depth of each tree\n",
    "    'eta': 0.1,                       # Learning rate\n",
    "    'subsample': 0.8,                 # Use 80% of data for training each tree\n",
    "    'colsample_bytree': 0.8,          # Use 80% of features for each tree\n",
    "    'eval_metric': 'rmse',            # Root Mean Squared Error as evaluation metric\n",
    "    'seed': 42                        # Set a random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "\n",
    "# 6. Plot actual vs predicted price\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)  # Diagonal line for perfect predictions\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted Price')\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
